# DevOps

Современный конспект по DevOps, Docker, Kubernetes и Ansible
1. Введение: зачем нужны современные подходы к ИТ-инфраструктуре?
Старый подход:
Разработчики, тестировщики и системные администраторы работали раздельно.
Всё делалось вручную, коммуникации между отделами были минимальны.
Для оффлайн-программ и редких обновлений это подходило, но с появлением веб-сервисов рынок потребовал частых и быстрых релизов.
Результат:
Возникла необходимость в новом подходе для ускорения выпуска ПО и повышения качества.
2. DevOps: история и суть
История появления
2007: Патрик Дебуа (Бельгия) сталкивается с проблемами между Dev и Ops.
2009: Первая DevOpsDays — отправная точка мирового движения.
Кто такие DevOps-инженеры?
DevOps-инженер — это специалист, который объединяет разработку, тестирование и эксплуатацию в единый автоматизированный процесс, создает и поддерживает инфраструктуру, автоматизирует рутинные задачи и обеспечивает быструю доставку ПО.
Методология DevOps
DevOps — это не только инструменты, но и культура совместной работы.
Задача: ускорить и упростить выпуск новых версий ПО.
Основные инструменты: системы контроля версий, CI/CD, мониторинг, контейнеризация, инфраструктура как код.
DevOps-инженер — наладчик коммуникаций и автоматизации.
Какие проблемы решает DevOps?
Медленные релизы (решается CI/CD и автоматизацией)
Плохое взаимодействие между отделами (единая среда и процессы)
Рутинные ручные задачи (автоматизация)
Недостаточная стабильность (мониторинг, логирование, тестирование)
Отсутствие гибкости (контейнеризация, IaC)
Кому нужен DevOps?
Разработчики — быстрее тестируют и выкатывают код.
Операционные команды — упрощают сопровождение.
Бизнес — быстрее выпускают новые фичи, увеличивают конкурентоспособность.
Безопасность — внедрение DevSecOps.
3. Базовые термины и концепции
Парсинг — автоматический сбор и структурирование информации.
Закон Конвея — структура коммуникаций в компании определяет архитектуру создаваемых систем.
Стейджинг — среда, максимально близкая к продакшену, для финальных тестов.
Теория ограничений — производительность системы определяется самым слабым звеном.
CALMS — фреймворк внедрения DevOps: Culture, Automation, Lean, Measurement, Sharing.
Стена замешательства — ситуация непонимания между Dev и Ops.
Бункерный менталитет — отсутствие коммуникаций между отделами.
Бизнес-ценность — соответствие/превышение ожиданий клиента (качество, скорость, затраты).
Декларативный подход — описываем, что хотим получить.
Императивный подход — описываем, как получить результат.
4. Docker: контейнеризация и файловые системы
Эволюция изоляции приложений
1979 — chroot: изолирует файловую систему.
2000 — FreeBSD jails.
2008 — LXC (Linux Containers): namespace + cgroups.
namespace — изоляция ресурсов. Каждому контейнеру выделяется свой собственный набор пространств имен, что делает его процессы, сеть и файловую систему изолированными от хоста и других контейнеров. Это включает изоляцию:
pid (Process ID): Каждый контейнер имеет свой независимый набор PID.
net (Network): Свой сетевой стек, включая сетевые интерфейсы, IP-адреса, таблицы маршрутизации, DNS.
mnt (Mount): Собственное пространство монтирования файловой системы.
uts (UNIX Time-sharing System): Изоляция имени хоста и доменного имени.
ipc (Interprocess Communication): Изоляция средств межпроцессного взаимодействия.
user (User ID): Изоляция пользовательских и групповых ID.
cgroups (Control Groups) — ограничение ресурсов. Позволяют Docker управлять тем, сколько ресурсов хоста может использовать контейнер (CPU, память, I/O, сеть), предотвращая монополизацию ресурсов одним контейнером.
Docker: как это работает?


2013 — dotCloud создает Docker.
Сначала использует LXC, затем собственный Libcontainer, а с 2016 — runc + containerd.
Пользователь → Docker CLI → dockerd → containerd → containerd-shim → runc → контейнер (процесс в namespaces + cgroups)(сопровождение контейнера).
Компоненты:
Docker CLI — интерфейс командной строки для взаимодействия с Docker Daemon. Пользователь вводит команды docker run, docker build и т.д.
Docker Daemon (dockerd) — фоновый процесс, работающий на хосте. Он отвечает за создание, запуск и управление Docker-объектами, такими как образы, контейнеры, сети и тома. Daemon слушает запросы от Docker CLI через REST API.
containerd — высокоуровневый демон, управляющий жизненным циклом контейнеров на хосте. Он берет на себя управление образами (pull/push), хранение (snapshotting), управление низкоуровневыми запусками контейнеров. containerd абстрагирует Docker Daemon от деталей низкоуровневого выполнения контейнеров.
runc — легковесная, универсальная утилита командной строки для запуска контейнеров в соответствии со спецификацией OCI (Open Container Initiative). runc фактически создает и запускает контейнер, используя namespace и cgroups ядра Linux.
shim — процесс-посредник между containerd и запущенным контейнером. Он позволяет containerd отсоединиться от runc после запуска контейнера, при этом сохраняя ввод/вывод контейнера и его статус. Shim обеспечивает стабильную работу контейнера, даже если containerd будет перезапущен.
OverlayFS, overlay2 и файловые системы в Docker
Основные понятия
ext4 — наиболее распространённая файловая система для раздела диска в Linux. На ней физически хранятся данные Docker (образы, контейнеры) в директории /var/lib/docker.
OverlayFS — технология из ядра Linux, позволяющая объединять несколько директорий (слоёв) в одну виртуальную файловую систему. Это реализация концепции UnionFS, которая предоставляет "объединенное представление" из нескольких слоев.
overlay2 — рекомендуемый драйвер хранения (storage driver) в Docker, использующий OverlayFS для эффективной работы со слоями контейнеров. Он является более продвинутой версией overlay драйвера и лучше справляется с инодами и многослойностью.
UnionFS — общее название для технологий, которые объединяют несколько файловых систем или директорий в единое логическое представление. Это позволяет изменять файловую систему без модификации базовых слоев, делая изменения только в верхнем, записываемом слое.
Как работает OverlayFS/overlay2 в Docker
OverlayFS не заменяет ext4! Она работает поверх базовой ФС (ext4, xfs и т.д.). Она создает логическое представление, а физические данные хранятся на базовой ФС.
Docker хранит образы и контейнеры в /var/lib/docker/, обычно находящемся на ext4. Эта директория содержит все слои образов и writable-слои контейнеров.
OverlayFS объединяет слои в итоговую файловую систему (merged directory), которую видит контейнер:
lowerdir (read-only) — содержит слои образа, которые не меняются.
upperdir (read-write) — сюда записываются все изменения, сделанные внутри контейнера.
workdir — временная директория, используемая OverlayFS для подготовки файлов перед перемещением их в upperdir.
overlay2 — современный драйвер Docker, использующий возможности OverlayFS:
Поддержка нескольких нижних слоёв (lowerdir), что позволяет эффективно работать с многослойными Docker-образами.
Корректная работа с hard links.
Эффективная работа с inode.
Используется по умолчанию (если поддерживается ядром: Linux 3.18+).
Проверить используемый драйвер:
docker info | grep "Storage Driver"
# Обычно вывод: Storage Driver: overlay2


Структура хранения слоёв
┌───────────────────────────────────────────┐
│              CONTAINER VIEW               │
│                                           │
│  (Виртуальное объединенное представление │
│   всех слоев, предоставляемое OverlayFS)  │
└─────────────────────▲─────────────────────┘
                      │
           (Читает/Записывает через)
                      │
┌─────────────────────┴─────────────────────┐
│  Writeable Layer (/<container-id>/diff/)  │  ← Все изменения контейнера (создание, удаление, запись файлов)
├───────────────────────────────────────────┤
│    Read-Only Image Layer 3 (RUN)          │  ← Результат команды RUN (например, установленные пакеты)
│    (/<layer-id-3>/diff/)                  │
├───────────────────────────────────────────┤
│    Read-Only Image Layer 2 (COPY)         │  ← Скопированные файлы приложения
│    (/<layer-id-2>/diff/)                  │
├──────────────────────────────────────────┤
│    Read-Only Image Layer 1 (FROM)         │  ← Базовый образ (ОС + Python)
│    (/<layer-id-1>/diff/)                  │
└─────────────────────┴─────────────────────┘
                      │
           (Все эти слои физически хранятся на)
                      │
┌─────────────────────▼─────────────────────┐
│         Файловая система хоста            │
│            (Например, ext4)               │
└───────────────────────────────────────────┘


Файлы физически лежат на ext4, но логически объединяются через overlayfs.
Почему Docker не использует ext4 напрямую?
ext4 не поддерживает наслоение (layers).
overlayfs экономит место за счёт недублирования одинаковых файлов в слоях и быстро запускает контейнеры (не нужно копировать всё, просто монтируются слои).
Изоляция изменений (upperdir для записи).
Поддерживаемые драйверы хранения в Docker/Containerd:
overlay2 (OverlayFS, основной для Linux)
aufs (устарел, медленнее, больше не рекомендуется)
btrfs/zfs (использует особенности одноимённых ФС)
devicemapper (раньше в RHEL/CentOS)
vfs (медленно, только для тестов)
fuse-overlayfs (для запуска без root-прав, через FUSE)
overlayfs, overlay2 и snapshotters
containerd поддерживает плагины snapshotters для хранения слоёв:
overlayfs (snapshotter) — основной, использует OverlayFS ядра Linux.
fuse-overlayfs — через FUSE, если нет прав на overlayfs.
aufs — устарел, не используется.
btrfs/zfs — для специфических случаев (если диск на btrfs/zfs).
stargz — для ленивой загрузки образов (Kubernetes, Google).
native — для Windows (NTFS).
Docker → containerd → выбранный snapshotter (по умолчанию overlayfs)
overlay2 в Docker — это по сути обёртка над overlayfs-снапшоттер в containerd.
Проверить snapshotter:
containerd config dump | grep -A 3 "snapshotter"
docker info | grep "Storage Driver"


OverlayFS и UnionFS: различия и эволюция
UnionFS — общий термин для технологий объединения слоёв (AUFS, OverlayFS и др.).
AUFS — старая реализация, требовала патчей ядра, медленнее, не поддерживается современными дистрибутивами.
OverlayFS — реализация UnionFS, встроенная в ядро Linux (3.18+), быстрая и поддерживаемая.
Пример схемы объединения:
Контейнер (merged)
│
├─ upperdir (запись изменений)
│
└─ lowerdir (слои образа, read-only)
   │
   └─ Всё хранится на ext4/btrfs (физический диск)


Кратко:
OverlayFS (snapshotter в containerd) и OverlayFS из ядра — это одно и то же (разница только в контексте применения).
overlay2 — драйвер Docker, использующий overlayfs-снапшоттер через containerd.
Для большинства сценариев overlay2/overlayfs — лучший и рекомендуемый выбор.
Docker-сети
Docker предоставляет несколько сетевых драйверов для подключения контейнеров к сети. Каждый драйвер имеет свои особенности и сценарии использования.
Типы сетей:
bridge (по умолчанию):
Это стандартный сетевой драйвер. Docker создает виртуальный мост (docker0 по умолчанию) на хосте. Все контейнеры, подключенные к этой сети, получают IP-адреса из диапазона этой подсети и могут взаимодействовать друг с другом.
Как работает bridge сеть:
Когда Docker Daemon запускается, он создает виртуальный сетевой мост с именем docker0. Этот мост работает как программный коммутатор.
Каждому контейнеру, который запускается с сетевым драйвером bridge (по умолчанию), назначается виртуальный Ethernet-интерфейс (например, eth0 внутри контейнера).
Этот виртуальный интерфейс подключается к мосту docker0 на хосте. С точки зрения хоста, для каждого контейнера создается пара виртуальных Ethernet-интерфейсов (veth pair), один конец которых находится внутри контейнера, а другой подключен к мосту docker0.
Docker также настраивает правила NAT (Network Address Translation) в iptables хоста, чтобы контейнеры могли получить доступ к внешнему миру и чтобы трафик извне мог быть перенаправлен на опубликованные порты контейнеров.
Контейнеры, находящиеся в одной bridge сети, могут общаться друг с другом по IP-адресам. Если используется пользовательская bridge сеть (созданная командой docker network create), то они также могут общаться по именам контейнеров, благодаря встроенному DNS-серверу Docker.
Контейнеры видят друг друга только по IP (в случае дефолтной сети bridge). В пользовательских сетях они могут обращаться друг к другу по имени.
host:
Контейнер полностью использует сетевой стек хоста. Он не имеет собственного изолированного сетевого пространства имен. Это означает, что контейнер напрямую использует IP-адреса и порты хоста.
Преимущество: высокая производительность, отсутствие NAT-преобразований.
Недостаток: теряется сетевая изоляция, возможны конфликты портов с приложениями на хосте.
Только для Linux.
none/null:
Контейнер не имеет сетевого интерфейса и не может взаимодействовать с внешним миром или другими контейнерами.
Используется для контейнеров, которым не требуется сеть (например, для выполнения пакетных задач).
overlay:
Используется для создания распределенных сетей между несколькими Docker-демонами на разных хостах (в Docker Swarm). Позволяет контейнерам на разных хостах общаться так, будто они находятся в одной локальной сети.
Требует наличия ключевого хранилища (key-value store) типа Consul, Etcd или ZooKeeper (или Swarm mode).
macvlan:
Позволяет контейнеру получить свой собственный MAC-адрес и IP-адрес из физической сети, делая его полноправным участником сети, как если бы это был отдельный физический хост.
Полезно, когда нужно, чтобы контейнеры напрямую взаимодействовали с физической сетью, без NAT.
Требует настройки сетевой карты хоста в promiscuous mode.
Пользовательские сети:
При создании своей сети Docker включает DNS — контейнеры видят друг друга по именам (сервис discovery). Это очень удобно для многокомпонентных приложений, где контейнеры могут обращаться друг к другу по имени сервиса, а не по динамическим IP-адресам.
Основные команды docker network:
docker network ls                                   # Список всех Docker-сетей
docker network create -d <драйвер> <имя>            # Создать новую сеть.
# Пример:
docker network create -d bridge my_custom_bridge_network # Пользовательская bridge-сеть
docker network create -d macvlan --subnet="192.168.1.0/24" --gateway="192.168.1.1" -o parent=eth0 my_macvlan_network # Macvlan сеть
docker inspect <имя_сети>                           # Детальная информация о сети (подсеть, шлюз, подключенные контейнеры)
docker network connect <сеть> <контейнер>           # Подключить запущенный контейнер к сети
docker network disconnect <сеть> <контейнер>        # Отключить контейнер от сети
docker network rm <имя_сети>                        # Удалить сеть (только если нет подключенных контейнеров)
docker network prune                                # Удалить все неиспользуемые сети


Другие полезные команды Docker:
# Работа с образами
docker pull <образ>:<тег>                           # Загрузить образ
docker images                                       # Список локальных образов
docker rmi <образ>:<тег>                            # Удалить образ
docker build . -t my_app:latest                     # Собрать образ из Dockerfile в текущей директории
docker build . -f Dockerfile.prod -t my_app:prod    # Собрать образ из указанного Dockerfile
docker image prune                                  # Удалить неиспользуемые образы
docker image history <образ>                        # Показать слои образа и команды, которые их создали
docker save -o my_app.tar my_app:latest             # Сохранить образ в TAR-файл
docker load -i my_app.tar                           # Загрузить образ из TAR-файла

# Работа с контейнерами
docker run -d -p 80:80 --name my_web_app my_app:latest # Запустить контейнер в фоновом режиме, пробросить порт 80, дать имя
docker ps                                           # Список запущенных контейнеров
docker ps -a                                        # Список всех контейнеров (включая остановленные)
docker stop <container_id/name>                     # Остановить контейнер
docker start <container_id/name>                    # Запустить остановленный контейнер
docker restart <container_id/name>                  # Перезапустить контейнер
docker rm <container_id/name>                       # Удалить контейнер (только остановленный)
docker rm -f <container_id/name>                    # Принудительно удалить контейнер
docker container prune                              # Удалить все остановленные контейнеры
docker logs <container_id/name>                     # Показать логи контейнера
docker logs -f <container_id/name>                  # Показать логи контейнера в реальном времени
docker exec -it <container_id/name> /bin/bash       # Запустить интерактивную оболочку внутри контейнера
docker top <container_id/name>                      # Показать запущенные процессы внутри контейнера
docker stats                                        # Показать статистику использования ресурсов контейнерами
docker inspect <container_id/name>                  # Детальная информация о контейнере (IP, настройки и т.д.)
docker port <container_id/name>                     # Показать проброшенные порты контейнера
docker update --restart=always <container_id/name>  # Обновить политику перезапуска контейнера

# Работа с томами (Volumes)
docker volume ls                                    # Список всех томов
docker volume create <имя_тома>                     # Создать новый том
docker volume inspect <имя_тома>                    # Детальная информация о томе
docker run -v my_data:/app/data my_app              # Подключить том к контейнеру
docker volume prune                                 # Удалить неиспользуемые тома

# Системные команды
docker system df                                    # Использование дискового пространства Docker'ом
docker system prune -a                              # Очистить все неиспользуемые образы, контейнеры, сети, тома


Примеры Dockerfile
1. Простой Dockerfile для Nginx с пользовательской страницей
# Использование официального образа Nginx в качестве базового
FROM nginx:latest

# Копирование пользовательской HTML-страницы в директорию Nginx
# Файл index.html должен находиться в той же директории, что и Dockerfile
COPY index.html /usr/share/nginx/html/index.html

# Порт, который будет слушать контейнер
EXPOSE 80

# Команда для запуска Nginx (эта команда по умолчанию уже есть в базовом образе Nginx)
CMD ["nginx", "-g", "daemon off;"]


2. Dockerfile для Python Flask приложения
# Используем официальный образ Python
FROM python:3.9-slim-buster

# Устанавливаем рабочую директорию внутри контейнера
WORKDIR /app

# Копируем файл зависимостей в контейнер и устанавливаем их
# Используем COPY --chown=user:group для установки владельца (лучшая практика безопасности)
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

# Копируем остальной код приложения
COPY . .

# Открываем порт, на котором будет работать Flask-приложение
EXPOSE 5000

# Запускаем приложение
CMD ["python", "app.py"]


app.py и requirements.txt должны находиться рядом с Dockerfile.
3. Multistage Dockerfile для Go-приложения
Multistage build позволяет значительно уменьшить размер итогового образа, используя один образ для сборки приложения и другой, минимальный, для его запуска.
# --- Этап сборки (Builder Stage) ---
# Используем образ с Go для компиляции приложения
FROM golang:1.20-alpine AS builder

# Устанавливаем рабочую директорию
WORKDIR /app

# Копируем исходный код приложения
COPY . .

# Компилируем приложение. "-o app" указывает имя исполняемого файла.
# CGO_ENABLED=0 и GOOS=linux необходимы для создания статически слинкованного бинарника для Linux.
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .

# --- Этап запуска (Runner Stage) ---
# Используем минимальный образ Alpine для запуска скомпилированного бинарника
FROM alpine:latest

# Устанавливаем рабочую директорию
WORKDIR /root/

# Копируем скомпилированный бинарник из этапа сборки
COPY --from=builder /app/app .

# Порт, который будет слушать приложение
EXPOSE 8080

# Запускаем приложение
CMD ["./app"]


В этом примере Go-приложение компилируется в первом этапе, а затем только готовый бинарник копируется в чистый и очень легкий образ Alpine, что значительно уменьшает итоговый размер образа.
5. Kubernetes: управление контейнерами
Что такое Kubernetes?
Система для автоматизации развёртывания, масштабирования и управления контейнерными приложениями.
Основные компоненты
Control Plane (Master Node)
api-server — точка входа для управления кластером. REST API, через который взаимодействуют все компоненты кластера и внешние пользователи.
scheduler — отслеживает новые Pod'ы без назначенного узла и выбирает для них узел, основываясь на доступных ресурсах, ограничениях и политиках.
controller manager — набор контроллеров. Например, ReplicationController (управляет ReplicaSet), EndpointsController, NodeController, ServiceAccountController.
etcd — распределенная, согласованная база данных "ключ-значение", в которой хранится все состояние кластера Kubernetes, его конфигурация и данные метаданных.
Data Plane (Worker Node)
kubelet — агент, который работает на каждом рабочем узле. Он общается с api-server, принимает определения Pod'ов и запускает их контейнеры через container runtime. Также мониторит состояние Pod'ов и узла.
kube-proxy — сетевой прокси, который обеспечивает сетевое подключение для Pod'ов и Service'ов. Он поддерживает сетевые правила на узлах, позволяя сетевому соединению доходить до Pod'ов изнутри или извне кластера (используя iptables или IPVS).
container runtime — программное обеспечение, которое отвечает за запуск контейнеров. Это может быть Docker, containerd, CRI-O и т.д.
Важные понятия:
lease — механизм выбора лидера среди узлов Control Plane (в случае HA-кластера) и для обновления статуса узла.
Round Robin — алгоритм балансировки нагрузки, часто используемый для распределения трафика между Pod'ами, к которым обращается Service.
Endpoints — объект Kubernetes, который хранит список IP-адресов и портов Pod'ов, предоставляющих сервис. kube-proxy использует этот список для маршрутизации трафика.
Pause-контейнер — "держит" namespace Pod'а. Это первый контейнер, который создается в каждом Pod'е. Он предоставляет общий сетевой и IPC (Inter-Process Communication) namespace для всех других контейнеров в этом Pod'е, позволяя им эффективно взаимодействовать и совместно использовать сетевые ресурсы.
QOS-класс (Quality of Service) — определяет приоритет Pod'ов по ресурсам. Используется планировщиком и kubelet для принятия решений о вытеснении Pod'ов при нехватке ресурсов.
BestEffort: Нет гарантированных ресурсов. Pod'ы этого класса будут первыми вытеснены при нехватке ресурсов.
Burstable: Есть запросы на ресурсы, но нет жестких ограничений. Могут использовать больше ресурсов, если они доступны, но будут вытеснены после Guaranteed Pod'ов.
Guaranteed: Запросы и лимиты на CPU и память равны. Эти Pod'ы имеют наивысший приоритет и будут вытеснены последними.
Calico — CNI-плагин (Container Network Interface) для сетевых политик и безопасности в Kubernetes. Он обеспечивает сетевое взаимодействие между Pod'ами, а также мощные сетевые политики для изоляции и контроля трафика.
OwnerReferences — отслеживание зависимостей между объектами Kubernetes. Позволяет контроллерам понимать, какие объекты "принадлежат" другим (например, ReplicaSet является владельцем Pod'ов, а Deployment является владельцем ReplicaSet).
PersistentVolume (PV) и PersistentVolumeClaim (PVC) — механизм для предоставления и потребления постоянного хранения данных в кластере Kubernetes.
PV: абстракция физического хранилища (например, NFS, iSCSI, облачное хранилище), предоставляемого администратором кластера.
PVC: запрос пользователя на определенный объем и тип хранилища. Kubernetes связывает PVC с доступным PV.
Сетевая модель Kubernetes
IP-адрес для каждого Pod'а: Каждый Pod в Kubernetes получает свой собственный уникальный IP-адрес в кластере. Это позволяет Pod'ам взаимодействовать друг с другом напрямую, без необходимости в NAT (Network Address Translation).
Flat network: Сетевая модель Kubernetes предполагает, что все Pod'ы могут общаться друг с другом напрямую, независимо от того, на каком узле они запущены. Это обычно реализуется с помощью CNI (Container Network Interface) плагинов.
Service Discovery: Kubernetes Service предоставляет стабильный сетевой доступ к набору Pod'ов. Service имеет свой собственный IP-адрес (кластерный IP) и DNS-имя, через которые другие Pod'ы могут обращаться к нему.
ClusterIP: Внутренний IP-адрес, доступный только внутри кластера.
NodePort: Открывает порт на каждом узле кластера, который перенаправляет трафик на Service.
LoadBalancer: Создает внешний балансировщик нагрузки (обычно в облачной среде), который маршрутизирует трафик к Service.
ExternalName: Сопоставляет Service с внешним DNS-именем.
CNI (Container Network Interface): Стандарт, определяющий интерфейс между платформой оркестрации контейнеров (Kubernetes) и сетевыми плагинами. CNI-плагины отвечают за настройку сети для Pod'ов. Примеры: Calico, Flannel, Weave Net, Cilium.
Работа с деплоями (kubectl)
kubectl -n <namespace> rollout history deploy/<deploy>    # история версий развертывания
kubectl -n <namespace> rollout undo deploy/<deploy> --to-revision=<n>   # откат развертывания к предыдущей версии
kubectl -n <namespace> rollout restart deploy/<deploy>    # рестарт всех Pod'ов в развертывании
kubectl -n <namespace> scale deploy/<deploy> --replicas=<n> # масштабирование количества реплик Pod'ов
kubectl -n <namespace> get po --show-labels               # показать Pod'ы с их лейблами
kubectl -n <namespace> edit deploy/<deploy>               # редактирование спецификации развертывания в реальном времени
kubectl -n <namespace> exec -it <pod> -- /bin/bash        # подключение к оболочке контейнера внутри Pod'а


Важно: Kubernetes создает новую версию deployment, если меняется спецификация Pod'а или ReplicaSet. Это позволяет отслеживать изменения и выполнять откаты.
Основные команды kubectl
# Общие команды
kubectl get <resource_type>                     # Получить список ресурсов (pods, deployments, services, nodes, namespaces, etc.)
kubectl get all                                 # Получить все основные ресурсы в текущем namespace
kubectl describe <resource_type> <name>         # Получить подробную информацию о ресурсе
kubectl logs <pod_name> [-c <container_name>]   # Просмотреть логи Pod'а (или конкретного контейнера в Pod'е)
kubectl logs -f <pod_name>                      # Следить за логами Pod'а в реальном времени
kubectl exec -it <pod_name> -- /bin/bash        # Выполнить команду в контейнере Pod'а (открыть интерактивную оболочку)
kubectl delete <resource_type> <name>           # Удалить ресурс
kubectl apply -f <file.yaml>                    # Создать или обновить ресурс из YAML-файла
kubectl create -f <file.yaml>                   # Создать ресурс из YAML-файла (не обновляет, если уже существует)
kubectl edit <resource_type>/<name>             # Редактировать живой ресурс в кластере
kubectl diff -f <file.yaml>                     # Показать различия между текущим состоянием ресурса в кластере и YAML-файлом
kubectl get events                              # Показать события кластера (полезно для траблшутинга)
kubectl top node                                # Показать использование ресурсов узлами
kubectl top pod                                 # Показать использование ресурсов Pod'ами (требует Metrics Server)
kubectl config view                             # Показать текущую конфигурацию kubeconfig
kubectl config use-context <context_name>       # Переключиться на другой контекст кластера
kubectl config current-context                  # Показать текущий контекст
kubectl taint nodes <node_name> <key>=<value>:<effect> # Добавить taint на узел (например, для изоляции)
kubectl cordon <node_name>                      # Отметить узел как unschedulable (но Pod'ы остаются)
kubectl drain <node_name>                       # Отметить узел как unschedulable и вытеснить все Pod'ы с него
kubectl uncordon <node_name>                    # Отменить unschedulable на узле
kubectl port-forward <pod_name> <local_port>:<container_port> # Проброс порта с локальной машины на Pod
kubectl cp <file_path> <pod_name>:/<destination_path> # Копирование файлов в Pod
kubectl cp <pod_name>:/<file_path> <local_destination_path> # Копирование файлов из Pod'а


Примеры манифестов Kubernetes (YAML)
1. Deployment: Развертывание Nginx приложения
Определяет желаемое состояние приложения (например, 3 реплики Nginx) и механизм их обновления.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3 # Желаемое количество реплик Pod'ов
  selector:
    matchLabels:
      app: nginx # Выбирает Pod'ы с этим лейблом
  template:
    metadata:
      labels:
        app: nginx # Лейбл для Pod'ов, созданных этим Deployment'ом
    spec:
      containers:
      - name: nginx # Имя контейнера
        image: nginx:latest # Используемый образ Docker
        ports:
        - containerPort: 80 # Порт, который слушает контейнер
        resources: # Определение запрашиваемых ресурсов и лимитов
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "200m"


2. Service: Открытие доступа к Nginx Deployment
Service предоставляет стабильную сетевую точку доступа к набору Pod'ов.
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx # Выбирает Pod'ы с лейблом app: nginx
  ports:
    - protocol: TCP
      port: 80 # Порт Service
      targetPort: 80 # Порт контейнера, на который перенаправляется трафик
      nodePort: 30000 # (Опционально) Если тип NodePort, порт на каждом узле. Должен быть в диапазоне 30000-32767.
  type: ClusterIP # Тип Service: ClusterIP (только внутри кластера), NodePort, LoadBalancer, ExternalName


3. Pod: Самая маленькая развертываемая единица
(Обычно Pod'ы создаются через Deployment/ReplicaSet, но можно создать и напрямую для тестов)
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox
    command: ["sh", "-c", "echo 'Hello from Kubernetes!' && sleep 3600"]


4. Namespace: Изоляция ресурсов
Для логической изоляции ресурсов кластера (например, для разных команд или сред).
apiVersion: v1
kind: Namespace
metadata:
  name: dev-environment


5. PersistentVolumeClaim (PVC): Запрос на хранилище
Пользовательский запрос на определенный объем и тип хранилища.
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce # Режим доступа: ReadWriteOnce, ReadOnlyMany, ReadWriteMany
  resources:
    requests:
      storage: 1Gi # Запросить 1 гигабайт хранилища


Для работы PVC должен быть настроен PersistentVolume (PV) или Dynamic Provisioning.
6. Ingress: Внешний доступ к Service по имени хоста/пути
Ingress управляет внешним доступом к сервисам в кластере, предоставляя маршрутизацию HTTP/HTTPS. Требует Ingress Controller (например, Nginx Ingress Controller).
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-app-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: / # Пример аннотации для Nginx Ingress Controller
spec:
  rules:
  - host: myapp.example.com # Доменное имя
    http:
      paths:
      - path: / # Путь, по которому будет доступен сервис
        pathType: Prefix
        backend:
          service:
            name: nginx-service # Имя Service, на который будет перенаправлен трафик
            port:
              number: 80 # Порт Service


Траблшутинг Kubernetes: полезные команды и подходы
Эффективный траблшутинг в Kubernetes требует систематического подхода. Начните с общего состояния, затем сужайте область поиска.
1. Проверка общего состояния кластера
kubectl get componentstatuses: Проверить состояние компонентов Control Plane (scheduler, controller-manager, etcd). В новых версиях Kubernetes может быть Deprecated или использоваться для старых кластеров. Лучше проверять по отдельности состояние подов Control Plane.
kubectl get nodes: Убедиться, что все узлы в статусе Ready. Если нет, то kubectl describe node <node_name>.
kubectl get pods --all-namespaces: Проверить, все ли Pod'ы запущены (Running) и без ошибок (CrashLoopBackOff, Evicted, Pending).
2. Траблшутинг Pod'ов
kubectl describe pod <pod_name>: Обязательная команда! Показывает очень подробную информацию о Pod'е:
Текущее состояние (Running, Pending, CrashLoopBackOff и т.д.)
Причины состояния (Events секция): почему Pod не запускается, вытеснен (Evicted), не может получить образ и т.д.
Использованные ресурсы (CPU/Memory requests/limits).
Узел, на котором запущен Pod.
Состояние контейнеров.
Подключенные тома.
В конце Events очень важен, он показывает историю событий, связанных с Pod'ом (попытки запуска, ошибки, вытеснения, сбои при загрузке образа).
kubectl logs <pod_name> [-c <container_name>]: Просмотр логов приложения внутри Pod'а.
-f: Следить за логами в реальном времени.
--tail=N: Показать последние N строк логов.
--since=1h: Показать логи за последний час.
kubectl exec -it <pod_name> -- /bin/bash (или sh, ash): Подключиться к контейнеру и выполнить команды внутри него (например, ping, curl, ls /app). Полезно, если приложение не отвечает или ведет себя неожиданно.
kubectl get pod <pod_name> -o yaml: Просмотреть полную YAML-спецификацию запущенного Pod'а, чтобы убедиться, что она соответствует ожиданиям.
kubectl delete pod <pod_name>: Удалить Pod. Если Pod управляется Deployment'ом, он будет автоматически перезапущен (полезно для "чистого" перезапуска).
3. Траблшутинг Deployment'ов и ReplicaSet'ов
kubectl describe deployment <deployment_name>: Проверить состояние Deployment'а. Обратить внимание на:
Количество желаемых/текущих/доступных реплик.
Стратегию обновления.
Events секцию для ошибок при развертывании.
kubectl rollout status deployment/<deployment_name>: Отслеживание статуса развертывания.
kubectl rollout history deployment/<deployment_name>: Посмотреть историю ревизий Deployment'а.
kubectl get replicasets: Проверить, что ReplicaSet, соответствующий Deployment'у, в порядке.
kubectl get pods -l app=<label_value>: Фильтрация Pod'ов по лейблам, чтобы увидеть, какие Pod'ы принадлежат конкретному Deployment'у или Service.
4. Траблшутинг Service'ов
kubectl describe service <service_name>: Посмотреть информацию о Service.
Endpoints: Самое главное — убедиться, что Service "видит" Pod'ы (есть IP-адреса Pod'ов в списке Endpoints). Если нет, значит, лейблы селектора Service не совпадают с лейблами Pod'ов, или Pod'ы не запущены.
Тип Service (ClusterIP, NodePort, LoadBalancer).
Порты.
kubectl get endpoints <service_name>: Быстро проверить, какие IP-адреса Pod'ов назначены Service.
Из другого Pod'а в кластере: Попробовать ping <service_name> (если в Pod'е есть ping), curl <service_name>:<port> для проверки доступности.
5. Траблшутинг сетевых проблем
kubectl get svc: Убедиться, что Service запущен и имеет корректный ClusterIP/NodePort/External IP.
kubectl get pods -o wide: Показывает IP-адреса Pod'ов.
kubectl describe node <node_name>: Проверить, что сетевой плагин CNI (например, Calico, Flannel) работает на узле.
kubectl logs <cni_pod_name> -n kube-system: Посмотреть логи CNI-плагина (например, kubectl logs calico-node-xyz -n kube-system).
kubectl exec -it <pod_name> -- netstat -tulnp: Проверить, слушает ли приложение внутри контейнера на ожидаемом порту.
kubectl get networkpolicy -A: Если используются NetworkPolicy, убедиться, что они не блокируют трафик.
ip route (на узле): Проверить таблицы маршрутизации, особенно если используется не-overlay CNI.
6. Траблшутинг узлов (Nodes)
kubectl describe node <node_name>: Показывает:
Состояние узла (Ready, NotReady).
Ресурсы узла и их использование.
Taints и Labels узла.
Events секция: ошибки kubelet, проблемы с диском, сетью.
Проверка системных логов на узле: journalctl -u kubelet, journalctl -u docker (или containerd), dmesg.
Проверка доступности дискового пространства на узле: df -h.
Проверка использования inode на узле: df -i.
7. Использование утилиты k9s
k9s: Интерактивная консольная утилита для управления и траблшутинга Kubernetes кластера. Позволяет удобно просматривать Pod'ы, логи, описывать ресурсы и многое другое. Очень рекомендуется для повседневной работы.
8. Распространенные проблемы и их решения
ImagePullBackOff: Контейнер не может загрузить образ.
Проверьте имя образа и тег.
Проверьте доступность реестра (Docker Hub, private registry).
Проверьте imagePullSecrets, если используется приватный реестр.
kubectl describe pod <pod_name> покажет детали ошибки.
CrashLoopBackOff: Приложение внутри контейнера падает сразу после запуска.
Самая частая проблема.
kubectl logs <pod_name>: Посмотрите логи приложения, чтобы понять причину падения.
kubectl describe pod <pod_name>: Проверьте Events и Readiness/Liveness probes.
kubectl exec -it <pod_name> -- /bin/bash: Зайдите в контейнер и попробуйте запустить команду вручную.
Pending Pod: Pod не может быть запланирован на узел.
kubectl describe pod <pod_name>: В секции Events будет указана причина:
Недостаточно ресурсов (CPU, Memory).
Отсутствие узлов, соответствующих nodeSelector/nodeAffinity/taints/tolerations.
Проблемы с PersistentVolumeClaim (не найден PV).
Service не направляет трафик:
Проверьте selector Service и labels Pod'ов – они должны совпадать.
Проверьте targetPort Service и порт, который слушает приложение в контейнере.
Проверьте kubectl get endpoints <service_name> – если Endpoints пуст, Service не видит Pod'ы.
Проверьте firewall на узлах.
Проверьте, что kube-proxy работает на узлах.
6. Ansible: автоматизация управления инфраструктурой
Control Node — управляющий сервер с установленным Ansible. С него запускаются все команды и плейбуки.
Managed Nodes — управляемые машины (серверы). На них не требуется установка агента, Ansible работает по SSH.
Inventory — файл (обычно в формате INI или YAML) со списком IP-адресов или доменных имен управляемых серверов, сгруппированных по ролям.
Пример INI-инвентори:
[webservers]
web1.example.com
web2.example.com

[dbservers]
db1.example.com
db2.example.com

[all:vars]
ansible_user=remote_user
ansible_ssh_private_key_file=~/.ssh/id_rsa


Пример YAML-инвентори:
all:
  hosts:
    web1.example.com:
    web2.example.com:
  children:
    webservers:
      hosts:
        web1.example.com:
        web2.example.com:
    dbservers:
      hosts:
        db1.example.com:
        db2.example.com:
  vars:
    ansible_user: remote_user
    ansible_ssh_private_key_file: ~/.ssh/id_rsa


Playbooks — YAML-файлы, которые описывают желаемое состояние целевых систем. Они содержат последовательность задач (tasks), которые Ansible должен выполнить.
Модули — инструменты для конкретных задач (копирование файлов, установка пакетов, управление сервисами, выполнение команд и т.д.). Ansible имеет сотни встроенных модулей.
Tasks — единичные действия, которые выполняются на управляемых узлах (например, установить пакет, скопировать файл).
Handlers — специальные задачи, которые запускаются только по явному уведомлению (notify) от других задач. Обычно используются для перезапуска сервисов после изменения конфигурационных файлов.
Roles — способ организации Playbooks и других связанных файлов (шаблонов, переменных) в логически структурированные и переиспользуемые единицы.
Idempotency — ключевой принцип Ansible. Плейбуки могут быть запущены многократно, и результат всегда будет одинаковым, независимо от исходного состояния системы. Ansible выполнит действие только если оно необходимо.
Variables — переменные, используемые для параметризации плейбуков (например, версии приложений, пути). Могут быть определены в инвентори, в плейбуках, в файлах vars/main.yml для ролей, или переданы через командную строку.
Facts — данные о целевых системах, собранные Ansible (например, IP-адреса, версия ОС, объем памяти). Используются для динамической настройки.
Templates (Jinja2) — шаблоны для создания конфигурационных файлов на целевых системах, используя переменные и факты.
Основные команды Ansible
# Проверка конфигурации и инвентори
ansible --version                               # Показать версию Ansible
ansible-inventory -i <inventory_file> --list    # Показать разобранный инвентори
ansible all -i <inventory_file> --list-hosts    # Список хостов из инвентори

# Выполнение Ad-Hoc команд (одноразовые команды без плейбуков)
ansible <group_name> -i <inventory_file> -m ping # Проверить связь со всеми хостами в группе
ansible all -i <inventory_file> -m command -a "uptime" # Выполнить команду на всех хостах
ansible webservers -i <inventory_file> -m apt -a "name=nginx state=present" --become # Установить Nginx на веб-серверах (с sudo)
ansible dbservers -i <inventory_file> -m shell -a "systemctl status postgresql" # Проверить статус сервиса PostgreSQL

# Выполнение Playbooks
ansible-playbook -i <inventory_file> <playbook_name.yml> # Запустить плейбук
ansible-playbook -i <inventory_file> <playbook_name.yml> --check # Проверить, что бы сделал плейбук, без реальных изменений
ansible-playbook -i <inventory_file> <playbook_name.yml> --diff # Показать изменения, которые будут внесены (с diff)
ansible-playbook -i <inventory_file> <playbook_name.yml> --syntax-check # Проверить синтаксис плейбука
ansible-playbook -i <inventory_file> <playbook_name.yml> --limit <host_or_group> # Запустить плейбук только на указанных хостах/группах
ansible-playbook -i <inventory_file> <playbook_name.yml> --start-at-task "<task_name>" # Начать выполнение с определенной задачи
ansible-playbook -i <inventory_file> <playbook_name.yml> -e "variable_name=value" # Передать переменную из командной строки
ansible-playbook -i <inventory_file> <playbook_name.yml> --tags "tag1,tag2" # Выполнить только задачи с указанными тегами
ansible-playbook -i <inventory_file> <playbook_name.yml> --skip-tags "tag_to_skip" # Пропустить задачи с указанными тегами


Пример Playbook: Установка Nginx и развертывание веб-страницы
---
- name: Configure Nginx Webserver
  hosts: webservers # Группа хостов из инвентори
  become: yes # Использовать sudo/root привилегии

  vars:
    nginx_port: 80
    nginx_docroot: "/var/www/html/my_website"

  tasks:
    - name: Ensure Nginx is installed
      ansible.builtin.apt: # Модуль для управления пакетами Debian/Ubuntu
        name: nginx
        state: present
        update_cache: yes

    - name: Ensure Nginx is running and enabled
      ansible.builtin.service: # Модуль для управления сервисами
        name: nginx
        state: started
        enabled: yes

    - name: Create web directory
      ansible.builtin.file: # Модуль для работы с файлами и директориями
        path: "{{ nginx_docroot }}"
        state: directory
        mode: '0755'

    - name: Copy index.html
      ansible.builtin.copy: # Модуль для копирования файлов
        src: files/index.html # Путь к файлу на Control Node
        dest: "{{ nginx_docroot }}/index.html" # Путь на Managed Node

    - name: Configure Nginx default site
      ansible.builtin.template: # Модуль для работы с шаблонами Jinja2
        src: templates/nginx.conf.j2 # Путь к файлу шаблона на Control Node
        dest: "/etc/nginx/sites-available/default"
      notify: Restart Nginx # Уведомить хендлер о необходимости перезапустить Nginx

  handlers:
    - name: Restart Nginx
      ansible.builtin.service:
        name: nginx
        state: restarted


files/index.html и templates/nginx.conf.j2 должны находиться относительно плейбука.
Пример templates/nginx.conf.j2:
server {
    listen {{ nginx_port }};
    listen [::]:{{ nginx_port }};
    root {{ nginx_docroot }};
    index index.html index.htm;
    server_name _;
    location / {
        try_files $uri $uri/ =404;
    }
}


7. Краткие шпаргалки
DevOps: ключевые принципы
Автоматизация всего (CI/CD, тесты, деплой, мониторинг)
Единая инфраструктура (IaC, Docker, Kubernetes)
Открытая коммуникация (Dev+Ops+QA)
Быстрая обратная связь
Docker: чтобы не забыть
Контейнер ≠ ВМ: меньше ресурсов, быстрее стартует, изолирован.
bridge — стандартная сеть, IP динамические.
host — контейнер использует сеть хоста.
none — сеть отключена.
overlay2 — рекомендуемый драйвер хранения, использующий OverlayFS (реализация UnionFS в ядре Linux)
OverlayFS — работает поверх ext4; объединяет слои lowerdir/upperdir/merged
containerd — управляет snapshotter'ами, overlayfs — основной snapshotter
AUFS — устарел и не рекомендуется
Kubernetes: основное
api-server — главный вход в кластер.
kubelet — связь между мастер- и воркер-нодами.
QOS — определяет приоритет Pod'ов.
Calico — сетевые политики и CNI-плагин.
PV/PVC — постоянное хранилище.
Каждый Pod получает свой IP.
Service обеспечивает доступ к Pod'ам.
kubectl describe и kubectl logs — ваши лучшие друзья для траблшутинга.
Ansible: основы
Всё в YAML.
Один управляющий сервер, много управляемых.
Playbooks — пошаговые сценарии.
Idempotency — повторные запуски безопасны.
Modules — для конкретных задач.
Roles — для организации и переиспользования кода.
Handlers — для выполнения действий только при изменениях.
8. Рекомендации для лучшего запоминания
Старайтесь не просто учить команды и определения, а понимать архитектуру и взаимосвязи между компонентами.
Практикуйтесь: создайте кластер Kubernetes локально (minikube/kind), разверните свои контейнеры с помощью Docker и опишите инфраструктуру через Ansible.
Используйте схемы, чтобы визуализировать процессы (например, жизненный цикл контейнера в Docker или поток данных в K8s).
Читайте документацию и следите за новыми инструментами и подходами в DevOps.
Этот конспект — ваша карта в мире современных IT-платформ.
